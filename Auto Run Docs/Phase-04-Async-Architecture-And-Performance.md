# Phase 04: Async Architecture and Performance

> **Branch Directive:** All work for this phase MUST be done on the `claude-overhaul` branch. Push commits to `origin/claude-overhaul` only. Do NOT push to `master` or `main`.

This phase replaces the legacy threading model with modern async/await patterns using asyncio, dramatically improving performance and resource efficiency. It adds progress reporting, concurrent file processing with configurable limits, and streaming output for large scans. The result is a scanner that handles massive codebases efficiently while providing real-time feedback to users.

## Tasks

- [x] Create `src/hamburglar/core/async_scanner.py` with an `AsyncScanner` class that: uses `asyncio.to_thread()` for file I/O operations, implements `asyncio.Semaphore` for concurrent file limit (default 50), provides async generator for streaming results, tracks progress (files scanned, files remaining, current file), supports cancellation via `asyncio.Event`
  - **Completed:** Created `AsyncScanner` class with full async/await support including:
    - `asyncio.to_thread()` for non-blocking file I/O operations
    - `asyncio.Semaphore` with configurable concurrency limit (default 50)
    - `scan_stream()` async generator for streaming findings in real-time
    - `ScanProgress` dataclass tracking total_files, scanned_files, current_file, bytes_processed, findings_count, elapsed_time, files_remaining
    - `asyncio.Event`-based cancellation via `cancel()` method and `is_cancelled` property
    - Added 36 comprehensive tests in `tests/test_async_scanner.py` covering all functionality
    - All 1820 tests pass with 95% coverage

- [x] Create `src/hamburglar/core/file_reader.py` with an `AsyncFileReader` class that: reads files asynchronously with configurable chunk size, detects encoding automatically (chardet/charset-normalizer), handles memory-mapped files for large file support, provides async context manager interface, implements file type detection (binary vs text)
  - **Completed:** Created `AsyncFileReader` class with comprehensive async file reading capabilities:
    - `asyncio.to_thread()` for non-blocking file I/O operations in thread pool
    - Configurable chunk size (default 8KB) with `read_chunks()` async generator
    - Automatic encoding detection using charset-normalizer with fallback heuristics
    - Memory-mapped file support via `mmap` module for large files (configurable threshold, default 10MB)
    - Full async context manager interface (`async with AsyncFileReader(...) as reader`)
    - File type detection with `FileType` enum (TEXT, BINARY, UNKNOWN) and UTF-16 BOM handling
    - `FileInfo` dataclass with path, file_type, encoding, size, and encoding_confidence
    - Convenience class methods: `read_file()`, `read_file_bytes()`, `is_binary()`, `is_text()`
    - Added charset-normalizer>=3.0.0 to pyproject.toml dependencies
    - Created 50 comprehensive tests in `tests/test_file_reader.py`
    - All 1870 tests pass with 93% coverage

- [x] Create `src/hamburglar/core/progress.py` with a `ScanProgress` dataclass (total_files, scanned_files, current_file, bytes_processed, findings_count, elapsed_time) and `ProgressReporter` protocol for pluggable progress reporting
  - **Completed:** Created `progress.py` module with comprehensive progress tracking capabilities:
    - `ScanProgress` dataclass with all required fields plus computed properties:
      - `files_remaining` (auto-computed)
      - `percent_complete` property (0-100%)
      - `files_per_second` throughput calculation
      - `bytes_per_second` throughput calculation
      - `estimated_time_remaining` property
      - `to_dict()` method for serialization
    - `ProgressReporter` protocol with `@runtime_checkable` decorator for pluggable reporting:
      - `on_progress(progress: ScanProgress)` for progress updates
      - `on_start(total_files: int)` for scan start notification
      - `on_complete(progress: ScanProgress)` for scan completion
      - `on_error(error: str, file_path: str | None)` for error reporting
    - `NullProgressReporter` class for no-op reporting (avoids None checks)
    - `CallbackProgressReporter` class for backward compatibility with callback-based reporting
    - Refactored `async_scanner.py` to import `ScanProgress` from `progress.py` (avoiding duplication)
    - Exported all classes from `hamburglar.core` package `__init__.py`
    - Created 42 comprehensive tests in `tests/test_progress.py` covering all functionality
    - All 1912 tests pass with 93% overall coverage (progress.py at 96%)

- [x] Create `src/hamburglar/outputs/streaming.py` with a `StreamingOutput` class that: yields findings as they're discovered, supports NDJSON (newline-delimited JSON) format, provides async iterator interface, allows real-time piping to other tools
  - **Completed:** Created comprehensive streaming output module with:
    - `StreamingOutput` class extending `BaseOutput` with NDJSON (newline-delimited JSON) format
    - `format_finding()` method for single finding JSON serialization
    - `stream_findings()` async generator for real-time streaming of findings
    - `write_stream()` method for direct output to file-like objects with optional flushing
    - `collect_findings()` method for collecting findings with optional max limit
    - `NDJSONStreamWriter` class for buffered output with backpressure handling:
      - Configurable buffer size (default 100)
      - Async context manager interface
      - `asyncio.Lock` for thread-safe buffering
      - `asyncio.to_thread()` for non-blocking writes
    - `stream_to_ndjson()` convenience function for easy NDJSON streaming
    - Exported all classes from `hamburglar.outputs` package
    - Created 37 comprehensive tests in `tests/test_streaming.py` covering:
      - NDJSON format correctness (valid JSON, single-line, no trailing commas)
      - Real-time streaming behavior
      - Stream interruption support
      - Backpressure handling
      - Integration with AsyncScanner
    - All 1949 tests pass with 94% overall coverage (streaming.py at 99%)

- [x] Update `src/hamburglar/cli/main.py` to: use `asyncio.run()` for the scan command, add `--concurrency/-j` option for parallel file limit (default 50), add `--stream` flag for streaming output mode, add rich progress bar using `rich.progress.Progress`, display real-time stats (files/sec, findings count)
  - **Completed:** Updated CLI with comprehensive async scanning support:
    - Replaced `Scanner` with `AsyncScanner` for all scanning operations
    - Added `--concurrency/-j` option (default 50, range 1-1000) for configurable parallelism
    - Added `--stream` flag for NDJSON streaming output mode with real-time findings
    - Implemented rich progress bar with `rich.progress.Progress` showing:
      - Spinner during file discovery phase
      - Progress bar with file count during scanning
      - Real-time stats: files/second throughput and findings count
    - Verbose mode shows detailed scan statistics including throughput
    - Streaming mode outputs NDJSON to stdout or file with immediate flushing
    - Created 27 comprehensive tests in `tests/test_cli_async.py` covering:
      - Concurrency option (valid values, invalid values, verbose display)
      - Streaming mode (NDJSON format, output to file, exit codes)
      - Progress bar functionality (quiet mode, verbose mode)
      - AsyncScanner integration (stats, duration, filters)
      - Combined options testing
    - All 1976 tests pass with 94% overall coverage

- [x] Create `src/hamburglar/core/file_filter.py` with a `FileFilter` class that: implements efficient glob pattern matching, supports gitignore-style patterns, caches compiled patterns for reuse, provides both sync and async interfaces
  - **Completed:** Created `FileFilter` class with comprehensive file filtering capabilities:
    - Efficient glob pattern matching using compiled regex (*, **, ?, [] patterns)
    - Full gitignore-style pattern support:
      - `!` prefix for negation patterns (re-include excluded files)
      - Trailing `/` for directory-only matching
      - Leading `/` for root-anchored patterns
      - `**` for matching any number of directory levels
    - `CompiledPattern` dataclass caching compiled regex patterns for reuse
    - Class-level pattern cache shared across instances
    - Both sync and async interfaces:
      - `should_include(path)` / `should_include_async(path)`
      - `filter_path(path)` / `filter_path_async(path)` with detailed `FilterResult`
      - `filter_paths(paths)` / `filter_paths_async(paths, concurrency_limit)`
    - Dynamic pattern management: `add_exclude()`, `add_include()`, `remove_exclude()`, `remove_include()`
    - `from_gitignore(path)` class method to load patterns from .gitignore files
    - Case-sensitive/insensitive matching support
    - Exported `FileFilter`, `FilterResult`, `CompiledPattern` from `hamburglar.core` package
    - Created 49 comprehensive tests in `tests/test_file_filter.py` covering:
      - Basic glob patterns (*, **, ?, [])
      - Gitignore-style features (negation, directory-only, anchored)
      - Include/exclude pattern interactions
      - Pattern caching and cache management
      - Async interface functionality
      - Real-world pattern scenarios (node_modules, .git, __pycache__, .env)
    - All 2025 tests pass with 93% coverage (file_filter.py at 93%)

- [x] Update `src/hamburglar/detectors/regex_detector.py` to: support async detect method, implement pattern caching (compile once), add timeout per-pattern to prevent catastrophic backtracking, batch pattern matching for efficiency
  - **Completed:** Updated `RegexDetector` with comprehensive async and batch detection capabilities:
    - Added `detect_async()` method using `asyncio.to_thread()` for non-blocking async detection
    - Pattern caching already implemented: patterns are compiled once at `__init__` and cached in `_compiled_patterns`
    - Timeout per-pattern already implemented via `_find_matches_with_timeout()` with chunk-based processing for large content
    - Added `detect_batch()` method for efficient batch detection of multiple files
    - Added `_detect_with_early_exit()` with `stop_on_first_match` option for quick scans
    - Added `detect_batch_async()` with configurable `concurrency_limit` and semaphore-based concurrency control
    - Added `get_pattern_stats()` method for pattern analytics by category, severity, and confidence
    - Created 37 comprehensive tests in `tests/test_regex_detector_async.py` covering:
      - Async detection (basic, concurrent, binary handling, file paths)
      - Batch detection (sync and async, concurrency control, early exit)
      - Pattern caching verification (thread safety, persistence across calls)
      - Timeout behavior in async context
      - Edge cases (unicode, null bytes, large batches, max file size)
      - Integration with AsyncScanner patterns
    - All 2062 tests pass with 4 warnings

- [x] Update `src/hamburglar/detectors/yara_detector.py` to: support async detect method via thread pool, implement rule caching, add scan timeout configuration, support streaming match results
  - **Completed:** Updated `YaraDetector` with comprehensive async and caching capabilities:
    - Added `detect_async()` method using `asyncio.to_thread()` for non-blocking async detection
    - Added `detect_bytes_async()` method for async binary content detection
    - Implemented class-level rule caching with SHA-256 hash of rule files:
      - `_RULE_CACHE` stores compiled rules with content hash and mtime for invalidation
      - `use_cache` parameter (default True) to enable/disable caching
      - `cache_key` property to access the current cache key
      - `get_cache_stats()` class method for cache statistics
      - `clear_cache()` class method to clear all cached rules
    - Scan timeout already implemented via YARA's native `timeout` parameter
    - Added `detect_stream()` async generator for streaming match results in real-time
    - Added `detect_batch()` for efficient batch detection of multiple files
    - Added `detect_batch_async()` with configurable `concurrency_limit` and semaphore-based concurrency
    - Added `get_detector_stats()` method for detector configuration analytics
    - `reload_rules()` now invalidates cache entry before recompiling
    - Created 49 comprehensive tests in `tests/test_yara_detector_async.py` covering:
      - Async detection (basic, concurrent, binary handling, file paths)
      - Async bytes detection (binary data, empty content, concurrent)
      - Batch detection (sync and async, concurrency control, large batches)
      - Streaming detection (single/multiple rules, metadata extraction)
      - Rule caching (enable/disable, reuse, invalidation, stats)
      - Timeout behavior in async context
      - Edge cases (unicode, null bytes, duplicate paths, max file size)
      - Integration with bundled YARA rules
      - Severity mapping and metadata extraction in async methods
    - All 2111 tests pass with 3 warnings

- [x] Create `src/hamburglar/core/stats.py` with a `ScanStats` class that tracks: total files scanned, total bytes processed, files skipped (and reasons), findings by detector, findings by severity, scan duration, files per second throughput
  - **Completed:** Created `ScanStats` class with comprehensive scan statistics tracking:
    - `ScanStats` dataclass with full statistics tracking:
      - `total_files_discovered`, `total_files_scanned`, `total_bytes_processed`
      - `skipped_files` list with `SkippedFile` objects containing file path, reason, and detail
      - `findings_by_detector` and `findings_by_severity` Counters for categorization
      - Timing: `scan_start_time`, `scan_end_time`, with `start()` and `stop()` methods
    - `SkipReason` enum with 11 reasons: permission_denied, file_not_found, read_error, blacklisted, not_whitelisted, binary_file, too_large, encoding_error, detector_error, cancelled, unknown
    - Computed properties: `scan_duration`, `files_per_second`, `bytes_per_second`, `total_files_skipped`, `total_findings`, `skipped_by_reason`, `is_running`, `is_complete`
    - Methods: `add_scanned_file()`, `add_skipped_file()`, `add_finding()`, `add_findings()`, `add_error()`, `reset()`, `merge()`, `__add__()` for combining stats
    - Serialization: `get_summary()`, `to_dict()` methods
    - Human-readable formatting: `format_duration()`, `format_bytes()`, `format_throughput()`, `__str__()`
    - Exported `ScanStats`, `SkipReason`, `SkippedFile` from `hamburglar.core` package
    - Created 46 comprehensive tests in `tests/test_stats.py` covering all functionality
    - All 2157 tests pass with 99% coverage on stats.py

- [x] Add memory profiling utilities in `src/hamburglar/core/profiling.py` with: optional memory tracking, peak memory usage reporting, per-detector timing stats, exportable performance report
  - **Completed:** Created comprehensive profiling module with:
    - `MemorySnapshot` dataclass for point-in-time memory usage (RSS, VMS, percent)
    - `MemoryProfiler` class for optional memory tracking with:
      - `start()` / `stop()` methods for profiling sessions
      - `snapshot(label)` for intermediate memory measurements
      - Peak memory tracking (RSS and VMS)
      - Memory delta calculation between start and end
      - Graceful degradation when psutil is unavailable
    - `TimingStats` dataclass for operation timing (total, count, min, max, avg)
    - `DetectorTimingStats` for per-detector timing with files processed and findings count
    - `PerformanceReport` dataclass aggregating all performance metrics:
      - Duration, files/second, bytes/second throughput
      - Memory profiling results
      - Per-detector timing breakdown
      - Custom timing categories
      - `to_dict()` and `to_json()` for exportable reports
    - `PerformanceProfiler` class with context managers:
      - `profile()` for overall scan profiling
      - `time_detector(name)` for detector timing with findings callback
      - `time_operation(name)` for custom operation timing
    - Utility functions: `format_bytes()`, `force_gc()`, `get_current_memory_rss()`, `is_memory_tracking_available()`, `@timed` decorator
    - Exported all classes from `hamburglar.core` package
    - Created 72 comprehensive tests in `tests/test_profiling.py`
    - All 2229 tests pass with 4 warnings

- [x] Create `tests/test_async_scanner.py` with tests for: async scanning produces same results as sync, concurrency limit is respected, cancellation works correctly, progress callbacks are called, streaming output works
  - **Completed:** Test file already exists with 36 comprehensive tests covering:
    - `TestAsyncScannerMatchesSyncResults`: async vs sync result comparison
    - `TestConcurrencyLimit`: semaphore-based concurrency control verification
    - `TestCancellation`: cancellation event and scan interruption
    - `TestProgressCallback`: progress tracking and callback invocation
    - `TestStreamingOutput`: async generator streaming functionality
    - Additional test classes for error handling, reset, blacklist/whitelist, non-recursive mode
    - All 36 tests pass with 0.31s execution time

- [x] Create `tests/test_file_reader.py` with tests for: async file reading works correctly, encoding detection works, large files are handled efficiently, binary file detection works, corrupt files don't crash reader
  - **Completed:** Test file already exists with 50 comprehensive tests covering all required areas:
    - `TestAsyncFileReaderBasic` (5 tests): text/bytes/empty file reading, file not found, directory error
    - `TestAsyncContextManager` (4 tests): open/close lifecycle, exception handling, manual open/close
    - `TestEncodingDetection` (4 tests): UTF-8, Latin-1, forced encoding, UTF-16 detection
    - `TestFileTypeDetection` (6 tests): text/binary detection, is_binary/is_text class methods, detect_type without opening
    - `TestChunkedReading` (4 tests): chunked reading, text chunks, custom chunk sizes
    - `TestMemoryMappedFiles` (4 tests): force mmap, force no mmap, threshold, mmap with chunks
    - `TestSeekOperations` (3 tests): seek to position, from end, relative seek
    - `TestFileInfo` (2 tests): file info attributes for text and binary files
    - `TestConvenienceMethods` (3 tests): read_file, read_file_bytes class methods
    - `TestErrorHandling` (6 tests): read/seek/chunk without open, permission denied, double open
    - `TestLargeFiles` (2 tests): 1MB file read, 500KB chunked read
    - `TestConcurrency` (2 tests): multiple readers same file, concurrent file reads
    - `TestEdgeCases` (5 tests): whitespace, unicode names, long lines, mixed line endings, high-bit characters
    - All 50 tests pass in 0.15s execution time

- [x] Create `tests/test_performance.py` with performance benchmarks: scan speed with 100 files, scan speed with 1000 files, memory usage stays bounded, concurrent scanning is faster than sequential
  - **Completed:** Created comprehensive performance benchmark test suite with 22 tests covering:
    - `TestScanSpeed100Files` (3 tests): completion, throughput (≥10 files/sec), findings detection
    - `TestScanSpeed1000Files` (4 tests): completion, throughput (≥20 files/sec), bytes processed, streaming
    - `TestMemoryBoundedness` (4 tests): memory bounded for 100/1000 files, peak tracking, no leaks on repeated scans
    - `TestConcurrencyPerformance` (4 tests): concurrent faster than sequential, limit variations, semaphore respected, no-detector speed
    - `TestBenchmarkMetrics` (3 tests): bytes/second, findings/file, duration reporting
    - `TestScalabilityBehavior` (4 tests): linear scaling, empty directory, large files, nested directories
    - All tests use the `MemoryProfiler` from `profiling.py` for memory tracking
    - Tests skip gracefully when `psutil` is not available for memory tracking
    - All 2248 tests pass with 3 skipped (memory tests when psutil unavailable)

- [x] Create `tests/test_streaming.py` with tests for: NDJSON output format is correct, findings stream as discovered, stream can be interrupted, backpressure is handled correctly
  - **Completed:** Test file already exists with 37 comprehensive tests covering all required areas:
    - `TestStreamingOutputBasic` (5 tests): basic functionality, empty results, single/multiple findings
    - `TestNDJSONFormat` (6 tests): single-line format, no trailing commas, valid JSON, special characters, unicode
    - `TestStreamFindings` (3 tests): async streaming, real-time yields, empty iterator handling
    - `TestStreamInterruption` (3 tests): early interruption, collection with limit, unlimited collection
    - `TestWriteStream` (3 tests): buffer writing, newline format, count return
    - `TestNDJSONStreamWriter` (6 tests): context manager, buffering, buffer size flush, write count, closed state, manual flush
    - `TestStreamToNdjson` (3 tests): convenience function for NDJSON output
    - `TestStreamingIntegration` (3 tests): integration with AsyncScanner, write stream, interrupt scanner
    - `TestStreamingEdgeCases` (5 tests): empty matches/metadata, complex metadata, large batch, severity levels
    - All 37 tests pass in 0.16s execution time

- [x] Update all existing tests to work with async scanner (use pytest-asyncio fixtures)
  - **Completed:** Updated 3 test files to use proper pytest-asyncio fixtures:
    - `test_binary_files.py`: Converted TestScannerMixedDirectories (3 async tests)
    - `test_large_files.py`: Converted TestMockedLargeFiles (2 tests) and TestScannerWithLargeFileSizeLimit (3 tests)
    - `test_encoding.py`: Converted all 30 test methods across 8 test classes
    - Removed `asyncio.run()` wrapper calls in favor of `@pytest.mark.asyncio` and `async def` patterns
    - Removed unused `asyncio` and `tempfile` imports
    - All 2248 tests pass with 3 skipped and 4 warnings
    - pytest-asyncio `asyncio_mode = "auto"` correctly auto-detects async tests

- [ ] Add `--benchmark` CLI flag that runs a quick performance test and reports files/second throughput

- [ ] Run pytest and ensure all tests pass with maintained 95%+ coverage, including async tests
